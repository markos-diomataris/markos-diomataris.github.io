---
layout: archive
title: "CV"
permalink: /cv/
author_profile: true
redirect_from:
  - /resume
---

{% include base_path %}

Research Interests
======
What does it take to create avatars that move and behave like real humans? My research focuses on building models and methods that try to answer this question. I want to create human avatars that not only move like us but also sense the world and learn from their experience like us. I combine data driven methods along with reinforcement learning paradigms in an effort to create agents that are able to learn human behaviors that go beyond what can be learned from the available datasets.

Education
======
* **Ph.D. Student**, ETH Zürich / Max Planck Institute Tübingen, CLS Program, 2022 - Present
  * Supervisors: [Michael Black](https://ps.is.mpg.de/person/black), [Otmar Hilliges](https://ait.ethz.ch/people/hilliges), and [Stelian Coros](https://crl.ethz.ch/people/coros/index.html)
  * Focus: Intelligent virtual humans, motion synthesis, reinforcement learning

* **Electrical and Computer Engineer**, National Technical University of Athens (NTUA), 2014 - 2020
  * **Thesis**: "Analyzing and Solving Context Bias in Visual Relationship Detection using Semi-Supervised Techniques"
  * Co-supervised by Prof. [Petros Maragos](http://cvsp.cs.ntua.gr/maragos/) and [Deeplab](https://deeplab.ai)

Work Experience
======
* **Computer Vision Researcher**, [Deeplab](https://deeplab.ai), 2020 - 2022
  * Leading the Vision & Language research team
  * Performing research on visual relationship detection
  * Supervising students and interns

Teaching Experience
======
* **Teaching Assistant**, ETH Zurich, Spring 2024
  * Course: Stochastics and Machine Learning (252-0870-00L)
  * Instructors: Carlos Cotrini Jimenez, Patrick Cheridito
  * Conducted exercise sessions, graded assignments, provided student support

Publications
======

**2025**
* **CLOPS: Moving By Looking** (arXiv 2025)
  * *Markos Diomataris*, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black

* **NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models** (arXiv 2025)
  * Mert Albaba, Chenhao Li, *Markos Diomataris*, Omid Taheri, Andreas Krause, Michael J. Black
  * [PDF](https://arxiv.org/abs/2503.10626) | [Project Page](https://mertalbaba.github.io/projects/1_nil/)

**2024**
* **WANDR: Intention-guided Human Motion Generation** (CVPR 2024)
  * *Markos Diomataris*, Nikos Athanasiou, Omid Taheri, Xi Wang, Otmar Hilliges, Michael J Black
  * [PDF](https://arxiv.org/pdf/2404.15383) | [Video](https://youtu.be/9szizM-XUCg?si=B836zQoWTI4I9s61) | [Website](https://wandr.is.tue.mpg.de/)

* **MotionFix: Text-Driven 3D Human Motion Editing** (arXiv 2024)
  * Nikos Athanasiou, Alpár Ceske, *Markos Diomataris*, Michael J. Black, Gül Varol
  * [PDF](https://arxiv.org/abs/2408.00712) | [Project Page](https://cure-lab.github.io/MotionFix/)

**2021**
* **Grounding Consistency: Distilling Spatial Common Sense for Precise Visual Relationship Detection** (ICCV 2021)
  * *Markos Diomataris*, Nikolaos Gkanatsios, Vassilis Pitsikalis, Petros Maragos
  * [PDF](https://openaccess.thecvf.com/content/ICCV2021/papers/Diomataris_Grounding_Consistency_Distilling_Spatial_Common_Sense_for_Precise_Visual_Relationship_ICCV_2021_paper.pdf) | [Video](https://www.youtube.com/watch?v=PQ9PH6do9Gc&t=2s) | [Demo](https://deeplab.ai/demo/vrd)

Skills
======
* **Programming Languages**: Python, C/C++, Matlab, Prolog
* **Frameworks**: PyTorch, Flask
* **Tools**: Git, Docker, ROS
* **Research Areas**: Computer Vision, Machine Learning, Motion Synthesis, Reinforcement Learning, Visual Relationship Detection

Contact
======
* **Email**: [diomataris.markos@gmail.com](mailto:diomataris.markos@gmail.com)
* **Google Scholar**: [Profile](https://scholar.google.com/citations?user=ImJYkBgAAAAJ&hl=en&oi=ao)
* **LinkedIn**: [markos-diomataris](https://www.linkedin.com/in/markos-diomataris-1b34881bb/)
