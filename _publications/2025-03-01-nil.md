---
title: "NIL: No-data Imitation Learning by Leveraging Pre-trained Video Diffusion Models"
collection: publications
permalink: /publication/nil
excerpt: 'NIL introduces a data-independent approach for motor skill acquisition that learns 3D motor skills from 2D-generated videos, with generalization capability to unconventional and non-human forms. We guide the imitation learning process by leveraging vision transformers for video-based comparisons and demonstrate that NIL outperforms baselines trained on 3D motion-capture data in humanoid robot locomotion tasks.'
venue: 'arXiv 2025'
authors: 'Mert Albaba, Chenhao Li, <strong>Markos Diomataris</strong>, Omid Taheri, Andreas Krause, Michael J. Black'
project_url: 'https://mertalbaba.github.io/projects/1_nil/'
pdf_url: 'https://arxiv.org/abs/2503.10626'
code_coming_soon: true
video_coming_soon: true
image: '/files/NIL.png'
---

NIL (No-data Imitation Learning) presents a novel approach to acquiring physically plausible motor skills across diverse morphologies without requiring extensive datasets. By leveraging pre-trained video diffusion models, our method generates reference videos from single frames and textual descriptions, then trains reinforcement learning policies to imitate these generated motions.

The approach consists of two stages: first, generating reference videos using video diffusion models conditioned on initial frames and task descriptions; second, training RL agents to imitate the generated videos through a reward function comprising video encoding similarity, segmentation mask IoU, and regularization terms.

Our method demonstrates superior performance compared to traditional approaches that rely on 3D motion-capture data, effectively replacing data collection with data generation for imitation learning across various morphologies including humanoid robots, quadrupeds, and other unconventional forms.