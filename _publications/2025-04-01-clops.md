---
title: "Moving by Looking: Towards Vision-Driven Avatar Motion Generation"
collection: publications
permalink: /projects/clops/
redirect_from:
  - /publication/clops
  - /publication/clops/
excerpt: 'Human-like motion requires human-like perception. We create a human motion generation system is purely driven by Vision.'
venue: 'arXiv 2025'
authors: '<strong>Markos Diomataris</strong>, Berat Mert Albaba, Giorgio Becherini, Partha Ghosh, Omid Taheri, Michael J. Black'
project_url: '/projects/clops/'
pdf_coming_soon: true
project_coming_soon: true
code_coming_soon: true
video_url: 'https://youtu.be/yny8DuObdMA'
video: '/files/clops.mp4'
layout: project
project_links:
  - label: "PDF"
    url: "#"
    type: "primary"
    icon: "fas fa-file-pdf"
  - label: "Code"
    url: "#"
    type: "inverse"
    icon: "fab fa-github"
  - label: "Video"
    url: "https://youtu.be/yny8DuObdMA"
    type: "danger" 
    icon: "fas fa-video"
---

<div class="project-video">
  <video autoplay muted loop playsinline preload="auto" controls>
    <source src="/files/clops_inout_cropped.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <!-- <p class="video-caption"><em>Detailed view of motion synthesis process and results.</em></p> -->
</div>

## What is CLOPS?

Human-Like motion requires human-like perception. We create a human motion generation system, named CLOPS, purely driven by egocentric visual observations. CLOPS is able to realistically move in a scene and use egocentric vision in order to find a goal (red sphere). We achieve this by combining a data driven low level motion prior with a Q-Learning policy that effectively create a loop of visual perception and motion.

<div class="project-video">
  <video autoplay muted loop playsinline preload="auto" controls>
    <source src="/files/clops_explanation.mp4" type="video/mp4">
    Your browser does not support the video tag.
  </video>
  <div class="audio-indicator">
    <i class="fas fa-volume-up" aria-hidden="true"></i>
    <span>This video includes audio narration</span>
  </div>
  <p class="video-caption"><em>Visual overview of CLOPS approach showing motion generation through visual observation.</em></p>
</div>


## Qualitative Results

The following examples demonstrate CLOPS in action. The Q-Network recieves egocentric observations at 1Hz and predicts goal poses for the avatar's head (visualised as coordinate frames). The motion generation network then generates natural motion in order to reach these head goals and the loop continues:

<div class="video-comparison">
  <div class="video-comparison-item">
    <video autoplay muted loop playsinline preload="auto" controls>
      <source src="/files/clops_example1.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div>
  
  <div class="video-comparison-item">
    <video autoplay muted loop playsinline preload="auto" controls>
      <source src="/files/clops_example2.mp4" type="video/mp4">
      Your browser does not support the video tag.
    </video>
  </div>
</div>

## Video

<div class="project-video">
  <div class="youtube-embed">
    <iframe src="https://www.youtube.com/embed/yny8DuObdMA" 
            frameborder="0" 
            allowfullscreen
            allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture">
    </iframe>
  </div>
  <!-- <p class="video-caption"><em>Comprehensive overview of the CLOPS approach and results.</em></p> -->
</div>

<!-- ## Citation

```bibtex
@article{diomataris2025clops,
  title={Moving by Looking: Towards Vision-Driven Avatar Motion Generation},
  author={Diomataris, Markos and Albaba, Berat Mert and Becherini, Giorgio and Ghosh, Partha and Taheri, Omid and Black, Michael J.},
  journal={arXiv preprint},
  year={2025}
}
``` -->
